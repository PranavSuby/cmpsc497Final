{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.04354405024983399,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00043544050249833986,
      "grad_norm": 0.36933329701423645,
      "learning_rate": 1.982e-05,
      "loss": 3.0756,
      "step": 10
    },
    {
      "epoch": 0.0008708810049966797,
      "grad_norm": 0.3453848361968994,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 3.0998,
      "step": 20
    },
    {
      "epoch": 0.0013063215074950196,
      "grad_norm": 0.3750453293323517,
      "learning_rate": 1.942e-05,
      "loss": 2.9872,
      "step": 30
    },
    {
      "epoch": 0.0017417620099933594,
      "grad_norm": 0.40450814366340637,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 2.9808,
      "step": 40
    },
    {
      "epoch": 0.0021772025124916993,
      "grad_norm": 0.4916997253894806,
      "learning_rate": 1.902e-05,
      "loss": 2.9186,
      "step": 50
    },
    {
      "epoch": 0.002612643014990039,
      "grad_norm": 0.4403379261493683,
      "learning_rate": 1.882e-05,
      "loss": 2.9758,
      "step": 60
    },
    {
      "epoch": 0.003048083517488379,
      "grad_norm": 0.477458119392395,
      "learning_rate": 1.862e-05,
      "loss": 2.9113,
      "step": 70
    },
    {
      "epoch": 0.003483524019986719,
      "grad_norm": 0.5154133439064026,
      "learning_rate": 1.8420000000000003e-05,
      "loss": 2.9342,
      "step": 80
    },
    {
      "epoch": 0.003918964522485059,
      "grad_norm": 0.48924005031585693,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 2.8767,
      "step": 90
    },
    {
      "epoch": 0.004354405024983399,
      "grad_norm": 0.4859066307544708,
      "learning_rate": 1.802e-05,
      "loss": 2.8839,
      "step": 100
    },
    {
      "epoch": 0.004789845527481739,
      "grad_norm": 0.49916693568229675,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 2.9155,
      "step": 110
    },
    {
      "epoch": 0.005225286029980078,
      "grad_norm": 0.5257850289344788,
      "learning_rate": 1.762e-05,
      "loss": 2.8849,
      "step": 120
    },
    {
      "epoch": 0.005660726532478419,
      "grad_norm": 0.4785521924495697,
      "learning_rate": 1.7420000000000003e-05,
      "loss": 2.8787,
      "step": 130
    },
    {
      "epoch": 0.006096167034976758,
      "grad_norm": 0.3952164649963379,
      "learning_rate": 1.722e-05,
      "loss": 2.7383,
      "step": 140
    },
    {
      "epoch": 0.006531607537475098,
      "grad_norm": 0.37262454628944397,
      "learning_rate": 1.702e-05,
      "loss": 2.7171,
      "step": 150
    },
    {
      "epoch": 0.006967048039973438,
      "grad_norm": 0.4658898413181305,
      "learning_rate": 1.682e-05,
      "loss": 2.8015,
      "step": 160
    },
    {
      "epoch": 0.007402488542471778,
      "grad_norm": 0.4248082637786865,
      "learning_rate": 1.662e-05,
      "loss": 2.7968,
      "step": 170
    },
    {
      "epoch": 0.007837929044970118,
      "grad_norm": 0.4147202670574188,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 2.7265,
      "step": 180
    },
    {
      "epoch": 0.008273369547468459,
      "grad_norm": 0.4560525417327881,
      "learning_rate": 1.6220000000000004e-05,
      "loss": 2.7708,
      "step": 190
    },
    {
      "epoch": 0.008708810049966797,
      "grad_norm": 0.36530184745788574,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 2.7166,
      "step": 200
    },
    {
      "epoch": 0.009144250552465137,
      "grad_norm": 0.5289048552513123,
      "learning_rate": 1.582e-05,
      "loss": 2.6719,
      "step": 210
    },
    {
      "epoch": 0.009579691054963478,
      "grad_norm": 0.44235390424728394,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 2.7316,
      "step": 220
    },
    {
      "epoch": 0.010015131557461818,
      "grad_norm": 0.5345929265022278,
      "learning_rate": 1.542e-05,
      "loss": 2.6923,
      "step": 230
    },
    {
      "epoch": 0.010450572059960157,
      "grad_norm": 0.4292672872543335,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 2.8223,
      "step": 240
    },
    {
      "epoch": 0.010886012562458497,
      "grad_norm": 0.38052475452423096,
      "learning_rate": 1.5020000000000002e-05,
      "loss": 2.6643,
      "step": 250
    },
    {
      "epoch": 0.011321453064956837,
      "grad_norm": 0.43983688950538635,
      "learning_rate": 1.482e-05,
      "loss": 2.7113,
      "step": 260
    },
    {
      "epoch": 0.011756893567455177,
      "grad_norm": 0.38730692863464355,
      "learning_rate": 1.462e-05,
      "loss": 2.74,
      "step": 270
    },
    {
      "epoch": 0.012192334069953516,
      "grad_norm": 0.4984269440174103,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 2.6904,
      "step": 280
    },
    {
      "epoch": 0.012627774572451856,
      "grad_norm": 0.5617188215255737,
      "learning_rate": 1.4220000000000001e-05,
      "loss": 2.6719,
      "step": 290
    },
    {
      "epoch": 0.013063215074950197,
      "grad_norm": 0.4652920365333557,
      "learning_rate": 1.402e-05,
      "loss": 2.6173,
      "step": 300
    },
    {
      "epoch": 0.013498655577448537,
      "grad_norm": 0.48675593733787537,
      "learning_rate": 1.382e-05,
      "loss": 2.7592,
      "step": 310
    },
    {
      "epoch": 0.013934096079946875,
      "grad_norm": 0.43599218130111694,
      "learning_rate": 1.3620000000000002e-05,
      "loss": 2.6268,
      "step": 320
    },
    {
      "epoch": 0.014369536582445216,
      "grad_norm": 0.47094500064849854,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 2.642,
      "step": 330
    },
    {
      "epoch": 0.014804977084943556,
      "grad_norm": 0.5732733011245728,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 2.746,
      "step": 340
    },
    {
      "epoch": 0.015240417587441896,
      "grad_norm": 0.4354931116104126,
      "learning_rate": 1.302e-05,
      "loss": 2.581,
      "step": 350
    },
    {
      "epoch": 0.015675858089940237,
      "grad_norm": 0.49029284715652466,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 2.6909,
      "step": 360
    },
    {
      "epoch": 0.016111298592438577,
      "grad_norm": 0.5098021626472473,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 2.5664,
      "step": 370
    },
    {
      "epoch": 0.016546739094936917,
      "grad_norm": 0.4646752178668976,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 2.6249,
      "step": 380
    },
    {
      "epoch": 0.016982179597435254,
      "grad_norm": 0.4436015784740448,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 2.5493,
      "step": 390
    },
    {
      "epoch": 0.017417620099933594,
      "grad_norm": 0.5748598575592041,
      "learning_rate": 1.202e-05,
      "loss": 2.6865,
      "step": 400
    },
    {
      "epoch": 0.017853060602431935,
      "grad_norm": 0.5182934403419495,
      "learning_rate": 1.182e-05,
      "loss": 2.6317,
      "step": 410
    },
    {
      "epoch": 0.018288501104930275,
      "grad_norm": 0.3973831236362457,
      "learning_rate": 1.162e-05,
      "loss": 2.6068,
      "step": 420
    },
    {
      "epoch": 0.018723941607428615,
      "grad_norm": 0.4657570719718933,
      "learning_rate": 1.142e-05,
      "loss": 2.6264,
      "step": 430
    },
    {
      "epoch": 0.019159382109926956,
      "grad_norm": 0.5186979174613953,
      "learning_rate": 1.1220000000000003e-05,
      "loss": 2.6575,
      "step": 440
    },
    {
      "epoch": 0.019594822612425296,
      "grad_norm": 0.4590088725090027,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 2.6502,
      "step": 450
    },
    {
      "epoch": 0.020030263114923636,
      "grad_norm": 0.46087926626205444,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 2.5972,
      "step": 460
    },
    {
      "epoch": 0.020465703617421973,
      "grad_norm": 0.46217095851898193,
      "learning_rate": 1.0620000000000002e-05,
      "loss": 2.5754,
      "step": 470
    },
    {
      "epoch": 0.020901144119920313,
      "grad_norm": 0.3642367124557495,
      "learning_rate": 1.0420000000000002e-05,
      "loss": 2.6223,
      "step": 480
    },
    {
      "epoch": 0.021336584622418654,
      "grad_norm": 0.43003949522972107,
      "learning_rate": 1.022e-05,
      "loss": 2.6836,
      "step": 490
    },
    {
      "epoch": 0.021772025124916994,
      "grad_norm": 0.5150102972984314,
      "learning_rate": 1.002e-05,
      "loss": 2.5481,
      "step": 500
    },
    {
      "epoch": 0.022207465627415334,
      "grad_norm": 0.5551819205284119,
      "learning_rate": 9.820000000000001e-06,
      "loss": 2.621,
      "step": 510
    },
    {
      "epoch": 0.022642906129913674,
      "grad_norm": 0.47021427750587463,
      "learning_rate": 9.620000000000001e-06,
      "loss": 2.6235,
      "step": 520
    },
    {
      "epoch": 0.023078346632412015,
      "grad_norm": 0.5008413791656494,
      "learning_rate": 9.42e-06,
      "loss": 2.5614,
      "step": 530
    },
    {
      "epoch": 0.023513787134910355,
      "grad_norm": 0.46037283539772034,
      "learning_rate": 9.220000000000002e-06,
      "loss": 2.6646,
      "step": 540
    },
    {
      "epoch": 0.023949227637408695,
      "grad_norm": 0.4566424489021301,
      "learning_rate": 9.020000000000002e-06,
      "loss": 2.5982,
      "step": 550
    },
    {
      "epoch": 0.024384668139907032,
      "grad_norm": 0.5393764972686768,
      "learning_rate": 8.82e-06,
      "loss": 2.6423,
      "step": 560
    },
    {
      "epoch": 0.024820108642405372,
      "grad_norm": 0.4285658597946167,
      "learning_rate": 8.62e-06,
      "loss": 2.5481,
      "step": 570
    },
    {
      "epoch": 0.025255549144903713,
      "grad_norm": 0.5217054486274719,
      "learning_rate": 8.42e-06,
      "loss": 2.5715,
      "step": 580
    },
    {
      "epoch": 0.025690989647402053,
      "grad_norm": 0.5126734972000122,
      "learning_rate": 8.220000000000001e-06,
      "loss": 2.6507,
      "step": 590
    },
    {
      "epoch": 0.026126430149900393,
      "grad_norm": 0.5509666800498962,
      "learning_rate": 8.020000000000001e-06,
      "loss": 2.5299,
      "step": 600
    },
    {
      "epoch": 0.026561870652398734,
      "grad_norm": 0.49282848834991455,
      "learning_rate": 7.820000000000001e-06,
      "loss": 2.6363,
      "step": 610
    },
    {
      "epoch": 0.026997311154897074,
      "grad_norm": 0.4770590364933014,
      "learning_rate": 7.620000000000001e-06,
      "loss": 2.6056,
      "step": 620
    },
    {
      "epoch": 0.027432751657395414,
      "grad_norm": 0.5613169074058533,
      "learning_rate": 7.420000000000001e-06,
      "loss": 2.6479,
      "step": 630
    },
    {
      "epoch": 0.02786819215989375,
      "grad_norm": 0.5139592885971069,
      "learning_rate": 7.22e-06,
      "loss": 2.6613,
      "step": 640
    },
    {
      "epoch": 0.02830363266239209,
      "grad_norm": 0.4524596929550171,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 2.6242,
      "step": 650
    },
    {
      "epoch": 0.02873907316489043,
      "grad_norm": 0.4798963963985443,
      "learning_rate": 6.820000000000001e-06,
      "loss": 2.6283,
      "step": 660
    },
    {
      "epoch": 0.029174513667388772,
      "grad_norm": 0.5135790705680847,
      "learning_rate": 6.620000000000001e-06,
      "loss": 2.5794,
      "step": 670
    },
    {
      "epoch": 0.029609954169887112,
      "grad_norm": 0.48211443424224854,
      "learning_rate": 6.42e-06,
      "loss": 2.5541,
      "step": 680
    },
    {
      "epoch": 0.030045394672385452,
      "grad_norm": 0.4831100106239319,
      "learning_rate": 6.220000000000001e-06,
      "loss": 2.5294,
      "step": 690
    },
    {
      "epoch": 0.030480835174883793,
      "grad_norm": 0.5483100414276123,
      "learning_rate": 6.02e-06,
      "loss": 2.5948,
      "step": 700
    },
    {
      "epoch": 0.030916275677382133,
      "grad_norm": 0.5020930171012878,
      "learning_rate": 5.82e-06,
      "loss": 2.4879,
      "step": 710
    },
    {
      "epoch": 0.03135171617988047,
      "grad_norm": 0.4491523802280426,
      "learning_rate": 5.620000000000001e-06,
      "loss": 2.5564,
      "step": 720
    },
    {
      "epoch": 0.03178715668237881,
      "grad_norm": 0.5346249341964722,
      "learning_rate": 5.420000000000001e-06,
      "loss": 2.5459,
      "step": 730
    },
    {
      "epoch": 0.032222597184877154,
      "grad_norm": 0.48897039890289307,
      "learning_rate": 5.220000000000001e-06,
      "loss": 2.5785,
      "step": 740
    },
    {
      "epoch": 0.03265803768737549,
      "grad_norm": 0.5211626887321472,
      "learning_rate": 5.02e-06,
      "loss": 2.5257,
      "step": 750
    },
    {
      "epoch": 0.033093478189873834,
      "grad_norm": 0.4698192775249481,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 2.5552,
      "step": 760
    },
    {
      "epoch": 0.03352891869237217,
      "grad_norm": 0.46372801065444946,
      "learning_rate": 4.620000000000001e-06,
      "loss": 2.5198,
      "step": 770
    },
    {
      "epoch": 0.03396435919487051,
      "grad_norm": 0.46707117557525635,
      "learning_rate": 4.42e-06,
      "loss": 2.5896,
      "step": 780
    },
    {
      "epoch": 0.03439979969736885,
      "grad_norm": 0.5356594920158386,
      "learning_rate": 4.22e-06,
      "loss": 2.5349,
      "step": 790
    },
    {
      "epoch": 0.03483524019986719,
      "grad_norm": 0.4945892095565796,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 2.4699,
      "step": 800
    },
    {
      "epoch": 0.03527068070236553,
      "grad_norm": 0.47558727860450745,
      "learning_rate": 3.820000000000001e-06,
      "loss": 2.5991,
      "step": 810
    },
    {
      "epoch": 0.03570612120486387,
      "grad_norm": 0.498514324426651,
      "learning_rate": 3.62e-06,
      "loss": 2.5427,
      "step": 820
    },
    {
      "epoch": 0.03614156170736221,
      "grad_norm": 0.4922572076320648,
      "learning_rate": 3.4200000000000007e-06,
      "loss": 2.5719,
      "step": 830
    },
    {
      "epoch": 0.03657700220986055,
      "grad_norm": 0.5186828374862671,
      "learning_rate": 3.2200000000000005e-06,
      "loss": 2.5593,
      "step": 840
    },
    {
      "epoch": 0.03701244271235889,
      "grad_norm": 0.5267392992973328,
      "learning_rate": 3.0200000000000003e-06,
      "loss": 2.5517,
      "step": 850
    },
    {
      "epoch": 0.03744788321485723,
      "grad_norm": 0.4451734721660614,
      "learning_rate": 2.82e-06,
      "loss": 2.5669,
      "step": 860
    },
    {
      "epoch": 0.03788332371735557,
      "grad_norm": 0.48765358328819275,
      "learning_rate": 2.6200000000000003e-06,
      "loss": 2.4702,
      "step": 870
    },
    {
      "epoch": 0.03831876421985391,
      "grad_norm": 0.4872131943702698,
      "learning_rate": 2.42e-06,
      "loss": 2.5367,
      "step": 880
    },
    {
      "epoch": 0.03875420472235225,
      "grad_norm": 0.48792901635169983,
      "learning_rate": 2.2200000000000003e-06,
      "loss": 2.5865,
      "step": 890
    },
    {
      "epoch": 0.03918964522485059,
      "grad_norm": 0.5378402471542358,
      "learning_rate": 2.02e-06,
      "loss": 2.4693,
      "step": 900
    },
    {
      "epoch": 0.03962508572734893,
      "grad_norm": 0.44046100974082947,
      "learning_rate": 1.8200000000000002e-06,
      "loss": 2.5661,
      "step": 910
    },
    {
      "epoch": 0.04006052622984727,
      "grad_norm": 0.5909598469734192,
      "learning_rate": 1.6200000000000002e-06,
      "loss": 2.6091,
      "step": 920
    },
    {
      "epoch": 0.04049596673234561,
      "grad_norm": 0.48780205845832825,
      "learning_rate": 1.42e-06,
      "loss": 2.5071,
      "step": 930
    },
    {
      "epoch": 0.040931407234843946,
      "grad_norm": 0.4692569971084595,
      "learning_rate": 1.2200000000000002e-06,
      "loss": 2.5826,
      "step": 940
    },
    {
      "epoch": 0.04136684773734229,
      "grad_norm": 0.5097525119781494,
      "learning_rate": 1.02e-06,
      "loss": 2.5243,
      "step": 950
    },
    {
      "epoch": 0.041802288239840626,
      "grad_norm": 0.5028528571128845,
      "learning_rate": 8.200000000000001e-07,
      "loss": 2.541,
      "step": 960
    },
    {
      "epoch": 0.04223772874233897,
      "grad_norm": 0.5195028781890869,
      "learning_rate": 6.200000000000001e-07,
      "loss": 2.547,
      "step": 970
    },
    {
      "epoch": 0.04267316924483731,
      "grad_norm": 0.5261044502258301,
      "learning_rate": 4.2000000000000006e-07,
      "loss": 2.5645,
      "step": 980
    },
    {
      "epoch": 0.04310860974733565,
      "grad_norm": 0.4699304699897766,
      "learning_rate": 2.2e-07,
      "loss": 2.5081,
      "step": 990
    },
    {
      "epoch": 0.04354405024983399,
      "grad_norm": 0.43243488669395447,
      "learning_rate": 2e-08,
      "loss": 2.5257,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2104152293376000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
